{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'c:\\\\users\\\\emrek\\\\source\\\\guidance\\\\guidance-pythonic\\\\guidance')\n",
    "\n",
    "import guidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environs import Env\n",
    "import os\n",
    "\n",
    "env = Env()\n",
    "env.read_env('./.env')\n",
    "\n",
    "# DONT FORGET TO CREATE THE .env file \n",
    "os.environ[\"OPENAI_API_TYPE\"] = env(\"OPENAI_API_TYPE\")\n",
    "# os.environ[\"OPENAI_API_VERSION\"] = env(\"OPENAI_API_VERSION\")\n",
    "# os.environ[\"OPENAI_API_BASE\"] = env(\"OPENAI_API_BASE\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='margin: 0px; padding: 0px; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'><div style='display: flex; border-bottom: 1px solid rgba(127, 127, 127, 0.2); align-items: center;'><div style='flex: 0 0 80px; opacity: 0.5;'>system</div><div style='flex-grow: 1; padding: 5px; padding-top: 10px; padding-bottom: 10px; margin-top: 0px; white-space: pre-wrap; margin-bottom: 0px;'>You are a helpful and terse assistant.</div></div><div style='display: flex; border-bottom: 1px solid rgba(127, 127, 127, 0.2); align-items: center;'><div style='flex: 0 0 80px; opacity: 0.5;'>user</div><div style='flex-grow: 1; padding: 5px; padding-top: 10px; padding-bottom: 10px; margin-top: 0px; white-space: pre-wrap; margin-bottom: 0px;'>Please tell me what the difference is between a frog and a toad.<div style='display: flex; border-bottom: 1px solid rgba(127, 127, 127, 0.2); align-items: center;'><div style='flex: 0 0 80px; opacity: 0.5;'>assistant</div><div style='flex-grow: 1; padding: 5px; padding-top: 10px; padding-bottom: 10px; margin-top: 0px; white-space: pre-wrap; margin-bottom: 0px;'><span style='background-color: rgba(0, 165, 0, 0.25)'>frogs are better</span></pre>"
      ],
      "text/plain": [
       "<guidance.models._openai.ChatOpenAI at 0x2517477e0a0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = guidance.models.ChatOpenAI(\"gpt-4\")\n",
    "\n",
    "lm = lm.system(\"You are a helpful and terse assistant.\")\n",
    "\n",
    "with lm.user() as lm:\n",
    "        lm += f\"\"\"Please tell me what the difference is between a frog and a toad.\"\"\"\n",
    "\n",
    "with lm.assistant() as lm:\n",
    "        #lm = lm.gen('answer', temperature=0, max_tokens=50)\n",
    "        lm = lm.select('answer', [\"toads are better\", \"frogs are better\"])\n",
    "        #lm = lm.select('answer', [\"toads are better\", \"frogs are better\", \"frogs are worse\"])\n",
    "        ## next line throws an error.\n",
    "        #lm = lm.select('answer', [\"frogs are better\", \"frogs are worse than toads\"])\n",
    "\n",
    "lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='margin: 0px; padding: 0px; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'>You are a helpful and terse assistant.\n",
       "Please tell me what the difference is between a frog and a toad. <span style='background-color: rgba(0, 165, 0, 0.25)'>Frogs are better</span></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm = guidance.models.Transformers(\"gpt2\", temperature=1.0)\n",
    "\n",
    "lm = lm(\"You are a helpful and terse assistant.\\n\")\n",
    "\n",
    "lm += f\"\"\"Please tell me what the difference is between a frog and a toad. \"\"\"\n",
    "\n",
    "#lm = lm.gen('answer', temperature=0, max_tokens=50)\n",
    "lm = lm.select('answer', [\"Toads are better\", \"Frogs are better\", \"Frogs are worse\"])\n",
    "#lm = lm.select('answer', [\"toads are better\", \"frogs are better\", \"frogs are worse\"])\n",
    "## next line throws an error.\n",
    "#lm = lm.select('answer', [\"frogs are better\", \"frogs are worse than toads\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'Frogs are better'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm._variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Constraint:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def set_context(self, prefix, lm):\n",
    "        pass\n",
    "\n",
    "    # TODO: switch interface to return next_char and make the shared code convert to tokens.\n",
    "    def valid_next_chars(self):  ## can return nothing if generation is complete\n",
    "        pass\n",
    "\n",
    "    def set_next_chars(self, c): ## returns a new immutable Constraint object with an updated internal state\n",
    "        return self\n",
    "\n",
    "class DummyCharacterConstraint(Constraint):\n",
    "\n",
    "    def __init__(self, positiveConstraint = ['a','b','c']):\n",
    "        self.positiveConstraint = positiveConstraint\n",
    "        #self.negativeConstraint = negativeConstraint\n",
    "    \n",
    "    def valid_next_chars(self):\n",
    "        return self.positiveConstraint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emrek\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "lm = guidance.models.Transformers(\"gpt2\", temperature=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_getnext_valid_token(constraint, validstr=\"\"):\n",
    "    ret = []\n",
    "    next_valid_chars = constraint.valid_next_chars()\n",
    "    if( len(next_valid_chars) > 0):\n",
    "        for c in next_valid_chars:\n",
    "            valid_tokens = lm.get_encoded(str(validstr + c))\n",
    "            if( len(valid_tokens) > 1):\n",
    "                #print(f\"valid token: {valid_tokens[0]}\")\n",
    "                ret.append(valid_tokens[0])\n",
    "            else:\n",
    "                # TODO -- need to update constraint with 'c' so that it has the right context\n",
    "                next_constraint = constraint.set_next_char(c)\n",
    "                ret.extend(recursive_getnext_valid_token(next_constraint, validstr + c))                \n",
    "        return set(ret)\n",
    "    else:\n",
    "        # TODO pull characters from suffix.\n",
    "        return []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid token: 24794\n",
      "valid token: 46071\n",
      "valid token: 7252\n",
      "valid token: 64\n",
      "valid token: 397\n",
      "valid token: 397\n",
      "valid token: 6485\n",
      "valid token: 6485\n",
      "valid token: 6485\n",
      "valid token: 7012\n",
      "valid token: 65\n",
      "valid token: 65\n",
      "valid token: 11848\n",
      "64 = a\n",
      "65 = b\n",
      "7012 = ba\n",
      "11848 = bb\n",
      "397 = ab\n",
      "7252 = aa\n",
      "6485 = abb\n",
      "46071 = aaa\n",
      "24794 = aaaa\n"
     ]
    }
   ],
   "source": [
    "\n",
    "next_valid_tokens = recursive_getnext_valid_token(DummyCharacterConstraint(positiveConstraint=['a', 'b']))\n",
    "\n",
    "for t in next_valid_tokens:\n",
    "    print(f\"{t} = {lm.get_decoded([t])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constrain(lm, name=\"value\", constraint=None, suffix=\"\", logprobs=None, list_append=False):\n",
    "    ''' Select a value from a list of choices.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    name : str\n",
    "        The name of the variable to set with the value generated under constraints.\n",
    "    constraint : Constraint\n",
    "        A Constraint object that .\n",
    "    suffix : str\n",
    "        An optional suffix to append to the selected value. Passing the next string as a suffix allows the select\n",
    "        statement to better differentiate between options that depend on the final token (and hence on a token that may overlap the following text).\n",
    "    list_append : bool\n",
    "        Whether to append the generated value to a list stored in the variable. If set to True, the variable\n",
    "        must be a list, and the generated value will be appended to the list.\n",
    "    '''\n",
    "    \n",
    "    assert constraint is not None, \"You must provide a constraint!\"\n",
    "\n",
    "    ### EMK: We will give the suffix to the constraint provider when asking for the next token\n",
    "    ###      When the constraint provider reaches the end of an option/program/etc, it will append the suffix    \n",
    "    def recursive_getnext_valid_token(constraint, validstr=\"\"):\n",
    "        ret = []\n",
    "        next_valid_chars = constraint.valid_next_chars()\n",
    "        if( len(next_valid_chars) > 0):\n",
    "            for c in next_valid_chars:\n",
    "                valid_tokens = lm.get_encoded(str(validstr + c))\n",
    "                if( len(valid_tokens) > 1):\n",
    "                    #print(f\"valid token: {valid_tokens[0]}\")\n",
    "                    ret.append(valid_tokens[0])\n",
    "                else:\n",
    "                    next_constraint = constraint.set_next_chars(c)\n",
    "                    ret.extend(recursive_getnext_valid_token(next_constraint, validstr + c))                \n",
    "            return set(ret)\n",
    "        else:\n",
    "            # TODO pull characters from suffix.\n",
    "            return []\n",
    "\n",
    "    def gen_next_token(current_prefix, valid_next_tokens):\n",
    "        logit_bias = {}\n",
    "        for token in valid_next_tokens:\n",
    "            logit_bias[token] = 100\n",
    "\n",
    "        print(f\"Current prefix = {current_prefix}\")\n",
    "        gen_obj = lm.get_endpoint_session()(\n",
    "            current_prefix, # TODO: perhaps we should allow passing of token ids directly? (this could allow us to avoid retokenizing the whole prefix many times)\n",
    "            max_tokens=1,\n",
    "            logit_bias=logit_bias,\n",
    "            logprobs=len(logit_bias),\n",
    "            cache_seed=0,\n",
    "            stream=False,\n",
    "            token_healing=False # we manage token boundary healing ourselves for this function\n",
    "        )\n",
    "        gen_obj = gen_obj[\"choices\"][0] # get the first choice (we only asked for one)\n",
    "        print(gen_obj)\n",
    "        return gen_obj[\"text\"] # TODO extend this to return the gen_obj\n",
    "\n",
    "    gen = str(lm)\n",
    "    current_constraint = constraint\n",
    "\n",
    "    # TODO extend this to do a beam search\n",
    "    for i in range(10):\n",
    "        next_valid_tokens = recursive_getnext_valid_token(current_constraint)\n",
    "        if( len(next_valid_tokens) == 0):\n",
    "            break\n",
    "\n",
    "        next_token = gen_next_token(gen, next_valid_tokens) \n",
    "        next_chars = next_token # the token is returned as characters, not as a token id\n",
    "        gen += next_chars\n",
    "        current_constraint = current_constraint.set_next_chars(next_chars)\n",
    "\n",
    "    return gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emrek\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "lm = guidance.models.Transformers(\"gpt2\", temperature=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='margin: 0px; padding: 0px; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'>Hello<span style='background-color: rgba(0, 165, 0, 0.25)'>, I&#x27;m sorry, but I&#x27;m not sure</span></pre>"
      ],
      "text/plain": [
       "<guidance.models._transformers.Transformers at 0x1a4aafb9700>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm(\"Hello\").gen('next', max_tokens=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from guidance import constrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from guidance.library import DummyCharacterConstraint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='margin: 0px; padding: 0px; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'>Hello<span style='background-color: rgba(0, 165, 0, 0.25)'>Hello, Wow, Wow! Woooo, Wow!</span></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm = lm(\"Hello\").constrain(name = \"var1\", constraint=DummyCharacterConstraint(positiveConstraint=[' ', ',', 'W', 'w', 'o', 'r', 'l', 'd', '!'])) # 'I', '\\'', 'm', 'p', 'g', 'a', 'm', 'B', 'o', 'b', "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, Wow, Wow! Woooo, Wow!'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm['var1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='margin: 0px; padding: 0px; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'>Hello<span style='background-color: rgba(0, 165, 0, 0.25)'>36903214341216103836</span></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from guidance.library import OddEvenDigitConstraint\n",
    "\n",
    "lm = guidance.models.Transformers(\"gpt2\", temperature=1.0)\n",
    "\n",
    "lm = lm(\"Hello\").constrain(name = \"var1\", constraint=OddEvenDigitConstraint(), max_tokens=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ContextFreeGrammarConstraint' from 'guidance.library.constraints' (c:\\users\\emrek\\source\\guidance\\guidance-pythonic\\guidance\\guidance\\library\\constraints\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\emrek\\source\\guidance\\guidance-pythonic\\guidance\\emrek\\test-pythonic.ipynb Cell 18\u001b[0m line \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/emrek/source/guidance/guidance-pythonic/guidance/emrek/test-pythonic.ipynb#X32sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m sys\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39minsert(\u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mc:\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39musers\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39memrek\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39msource\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mguidance\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mguidance-pythonic\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mguidance\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/emrek/source/guidance/guidance-pythonic/guidance/emrek/test-pythonic.ipynb#X32sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mguidance\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/emrek/source/guidance/guidance-pythonic/guidance/emrek/test-pythonic.ipynb#X32sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mguidance\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlibrary\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconstraints\u001b[39;00m \u001b[39mimport\u001b[39;00m ContextFreeGrammarConstraint\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'ContextFreeGrammarConstraint' from 'guidance.library.constraints' (c:\\users\\emrek\\source\\guidance\\guidance-pythonic\\guidance\\guidance\\library\\constraints\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'c:\\\\users\\\\emrek\\\\source\\\\guidance\\\\guidance-pythonic\\\\guidance')\n",
    "import guidance\n",
    "from guidance.library.constraints import ContextFreeGrammarConstraint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from guidance.library import Constraint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='margin: 0px; padding: 0px; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'>Hello<span style='background-color: rgba(0, 165, 0, 0.25)'>if (5) then X=1</span></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "lm = guidance.models.Transformers(\"gpt2\", temperature=1.0)\n",
    "\n",
    "testCFG = ContextFreeGrammarConstraint()\n",
    "testCFG.add_grammar_rule(testCFG.Rule(\"S\", [\"@i\", \"@f\", \"@ \", \"E\", \"@ \", \"@t\", \"@h\", \"@e\", \"@n\", \"@ \", \"C\"]))\n",
    "testCFG.add_grammar_rule(testCFG.Rule(\"E\", [\"@1\"]))\n",
    "testCFG.add_grammar_rule(testCFG.Rule(\"E\", [\"@2\"]))\n",
    "testCFG.add_grammar_rule(testCFG.Rule(\"E\", [\"@3\"]))\n",
    "testCFG.add_grammar_rule(testCFG.Rule(\"E\", [\"@4\"]))\n",
    "testCFG.add_grammar_rule(testCFG.Rule(\"E\", [\"@5\"]))\n",
    "testCFG.add_grammar_rule(testCFG.Rule(\"E\", [\"@6\"]))\n",
    "testCFG.add_grammar_rule(testCFG.Rule(\"E\", [\"@7\"]))\n",
    "testCFG.add_grammar_rule(testCFG.Rule(\"E\", [\"@8\"]))\n",
    "testCFG.add_grammar_rule(testCFG.Rule(\"E\", [\"@9\"]))\n",
    "testCFG.add_grammar_rule(testCFG.Rule(\"E\", [\"@(\", \"E\", \"@)\"]))\n",
    "testCFG.add_grammar_rule(testCFG.Rule(\"E\", [\"E\", \"@+\", \"E\"]))\n",
    "testCFG.add_grammar_rule(testCFG.Rule(\"E\", [\"E\", \"@-\", \"E\"]))\n",
    "testCFG.add_grammar_rule(testCFG.Rule(\"E\", [\"E\", \"@*\", \"E\"]))\n",
    "testCFG.add_grammar_rule(testCFG.Rule(\"E\", [\"E\", \"@/\", \"E\"]))\n",
    "testCFG.add_grammar_rule(testCFG.Rule(\"C\", [\"@X\", \"@=\", \"E\"]))\n",
    "testCFG.finalize_grammar()\n",
    "\n",
    "lm = lm(\"Hello\").constrain(name = \"var1\", constraint=testCFG, max_tokens=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_lm = guidance.models.Transformers(\"gpt2\", temperature=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='margin: 0px; padding: 0px; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'>If 2+4 then return 8: <span style='background-color: rgba(0, 165, 0, 0.25)'>if 1 then X=6</span></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm = init_lm(\"If 2+4 then return 8: \").constrain(name = \"var1\", constraint=testCFG, max_tokens=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E -> ['@1']\n"
     ]
    }
   ],
   "source": [
    "rules = testCFG.mtable[('E', '1')]\n",
    "for r in rules:\n",
    "    print(f\"{r.symbol} -> {r.productions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
