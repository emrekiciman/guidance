{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'c:\\\\users\\\\emrek\\\\source\\\\guidance\\\\guidance-pythonic\\\\guidance')\n",
    "\n",
    "import guidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environs import Env\n",
    "import os\n",
    "\n",
    "env = Env()\n",
    "env.read_env('./.env')\n",
    "\n",
    "# DONT FORGET TO CREATE THE .env file \n",
    "os.environ[\"OPENAI_API_TYPE\"] = env(\"OPENAI_API_TYPE\")\n",
    "# os.environ[\"OPENAI_API_VERSION\"] = env(\"OPENAI_API_VERSION\")\n",
    "# os.environ[\"OPENAI_API_BASE\"] = env(\"OPENAI_API_BASE\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='margin: 0px; padding: 0px; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'><div style='display: flex; border-bottom: 1px solid rgba(127, 127, 127, 0.2); align-items: center;'><div style='flex: 0 0 80px; opacity: 0.5;'>system</div><div style='flex-grow: 1; padding: 5px; padding-top: 10px; padding-bottom: 10px; margin-top: 0px; white-space: pre-wrap; margin-bottom: 0px;'>You are a helpful and terse assistant.</div></div><div style='display: flex; border-bottom: 1px solid rgba(127, 127, 127, 0.2); align-items: center;'><div style='flex: 0 0 80px; opacity: 0.5;'>user</div><div style='flex-grow: 1; padding: 5px; padding-top: 10px; padding-bottom: 10px; margin-top: 0px; white-space: pre-wrap; margin-bottom: 0px;'>Please tell me what the difference is between a frog and a toad.</div></div><div style='display: flex; border-bottom: 1px solid rgba(127, 127, 127, 0.2); align-items: center;'><div style='flex: 0 0 80px; opacity: 0.5;'>assistant</div><div style='flex-grow: 1; padding: 5px; padding-top: 10px; padding-bottom: 10px; margin-top: 0px; white-space: pre-wrap; margin-bottom: 0px;'></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logit_bias': {998: 100, 69: 100}, 'logprobs': 2, 'cache_seed': 0, 'stream': False, 'token_healing': False}\n"
     ]
    }
   ],
   "source": [
    "lm = guidance.models.OpenAIChat(\"gpt-4\")\n",
    "\n",
    "lm = lm.system(\"You are a helpful and terse assistant.\")\n",
    "\n",
    "with lm.user() as lm:\n",
    "        lm += f\"\"\"Please tell me what the difference is between a frog and a toad.\"\"\"\n",
    "\n",
    "with lm.assistant() as lm:\n",
    "        #lm = lm.gen('answer', temperature=0, max_tokens=50)\n",
    "        lm = lm.select('answer', [\"toads are better\", \"frogs are better\"])\n",
    "        #lm = lm.select('answer', [\"toads are better\", \"frogs are better\", \"frogs are worse\"])\n",
    "        ## next line throws an error.\n",
    "        #lm = lm.select('answer', [\"frogs are better\", \"frogs are worse than toads\"])\n",
    "\n",
    "lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='margin: 0px; padding: 0px; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'>You are a helpful and terse assistant.\n",
       "Please tell me what the difference is between a frog and a toad. <span style='background-color: rgba(0, 165, 0, 0.25)'>Frogs are better</span></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm = guidance.models.Transformers(\"gpt2\", temperature=1.0)\n",
    "\n",
    "lm = lm(\"You are a helpful and terse assistant.\\n\")\n",
    "\n",
    "lm += f\"\"\"Please tell me what the difference is between a frog and a toad. \"\"\"\n",
    "\n",
    "#lm = lm.gen('answer', temperature=0, max_tokens=50)\n",
    "lm = lm.select('answer', [\"Toads are better\", \"Frogs are better\", \"Frogs are worse\"])\n",
    "#lm = lm.select('answer', [\"toads are better\", \"frogs are better\", \"frogs are worse\"])\n",
    "## next line throws an error.\n",
    "#lm = lm.select('answer', [\"frogs are better\", \"frogs are worse than toads\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'Frogs are better'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm._variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Constraint:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def set_context(self, prefix, lm):\n",
    "        pass\n",
    "\n",
    "    # TODO: switch interface to return next_char and make the shared code convert to tokens.\n",
    "    def valid_next_chars(self):  ## can return nothing if generation is complete\n",
    "        pass\n",
    "\n",
    "    def set_next_chars(self, c): ## returns a new immutable Constraint object with an updated internal state\n",
    "        return self\n",
    "\n",
    "class DummyCharacterConstraint(Constraint):\n",
    "\n",
    "    def __init__(self, positiveConstraint = ['a','b','c']):\n",
    "        self.positiveConstraint = positiveConstraint\n",
    "        #self.negativeConstraint = negativeConstraint\n",
    "    \n",
    "    def valid_next_chars(self):\n",
    "        return self.positiveConstraint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emrek\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "lm = guidance.models.Transformers(\"gpt2\", temperature=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_getnext_valid_token(constraint, validstr=\"\"):\n",
    "    ret = []\n",
    "    next_valid_chars = constraint.valid_next_chars()\n",
    "    if( len(next_valid_chars) > 0):\n",
    "        for c in next_valid_chars:\n",
    "            valid_tokens = lm.get_encoded(str(validstr + c))\n",
    "            if( len(valid_tokens) > 1):\n",
    "                #print(f\"valid token: {valid_tokens[0]}\")\n",
    "                ret.append(valid_tokens[0])\n",
    "            else:\n",
    "                # TODO -- need to update constraint with 'c' so that it has the right context\n",
    "                next_constraint = constraint.set_next_char(c)\n",
    "                ret.extend(recursive_getnext_valid_token(next_constraint, validstr + c))                \n",
    "        return set(ret)\n",
    "    else:\n",
    "        # TODO pull characters from suffix.\n",
    "        return []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid token: 24794\n",
      "valid token: 46071\n",
      "valid token: 7252\n",
      "valid token: 64\n",
      "valid token: 397\n",
      "valid token: 397\n",
      "valid token: 6485\n",
      "valid token: 6485\n",
      "valid token: 6485\n",
      "valid token: 7012\n",
      "valid token: 65\n",
      "valid token: 65\n",
      "valid token: 11848\n",
      "64 = a\n",
      "65 = b\n",
      "7012 = ba\n",
      "11848 = bb\n",
      "397 = ab\n",
      "7252 = aa\n",
      "6485 = abb\n",
      "46071 = aaa\n",
      "24794 = aaaa\n"
     ]
    }
   ],
   "source": [
    "\n",
    "next_valid_tokens = recursive_getnext_valid_token(DummyCharacterConstraint(positiveConstraint=['a', 'b']))\n",
    "\n",
    "for t in next_valid_tokens:\n",
    "    print(f\"{t} = {lm.get_decoded([t])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constrain(lm, name=\"value\", constraint=None, suffix=\"\", logprobs=None, list_append=False):\n",
    "    ''' Select a value from a list of choices.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    name : str\n",
    "        The name of the variable to set with the value generated under constraints.\n",
    "    constraint : Constraint\n",
    "        A Constraint object that .\n",
    "    suffix : str\n",
    "        An optional suffix to append to the selected value. Passing the next string as a suffix allows the select\n",
    "        statement to better differentiate between options that depend on the final token (and hence on a token that may overlap the following text).\n",
    "    list_append : bool\n",
    "        Whether to append the generated value to a list stored in the variable. If set to True, the variable\n",
    "        must be a list, and the generated value will be appended to the list.\n",
    "    '''\n",
    "    \n",
    "    assert constraint is not None, \"You must provide a constraint!\"\n",
    "\n",
    "    ### EMK: We will give the suffix to the constraint provider when asking for the next token\n",
    "    ###      When the constraint provider reaches the end of an option/program/etc, it will append the suffix    \n",
    "    def recursive_getnext_valid_token(constraint, validstr=\"\"):\n",
    "        ret = []\n",
    "        next_valid_chars = constraint.valid_next_chars()\n",
    "        if( len(next_valid_chars) > 0):\n",
    "            for c in next_valid_chars:\n",
    "                valid_tokens = lm.get_encoded(str(validstr + c))\n",
    "                if( len(valid_tokens) > 1):\n",
    "                    #print(f\"valid token: {valid_tokens[0]}\")\n",
    "                    ret.append(valid_tokens[0])\n",
    "                else:\n",
    "                    next_constraint = constraint.set_next_chars(c)\n",
    "                    ret.extend(recursive_getnext_valid_token(next_constraint, validstr + c))                \n",
    "            return set(ret)\n",
    "        else:\n",
    "            # TODO pull characters from suffix.\n",
    "            return []\n",
    "\n",
    "    def gen_next_token(current_prefix, valid_next_tokens):\n",
    "        logit_bias = {}\n",
    "        for token in valid_next_tokens:\n",
    "            logit_bias[token] = 100\n",
    "\n",
    "        print(f\"Current prefix = {current_prefix}\")\n",
    "        gen_obj = lm.get_endpoint_session()(\n",
    "            current_prefix, # TODO: perhaps we should allow passing of token ids directly? (this could allow us to avoid retokenizing the whole prefix many times)\n",
    "            max_tokens=1,\n",
    "            logit_bias=logit_bias,\n",
    "            logprobs=len(logit_bias),\n",
    "            cache_seed=0,\n",
    "            stream=False,\n",
    "            token_healing=False # we manage token boundary healing ourselves for this function\n",
    "        )\n",
    "        gen_obj = gen_obj[\"choices\"][0] # get the first choice (we only asked for one)\n",
    "        print(gen_obj)\n",
    "        return gen_obj[\"text\"] # TODO extend this to return the gen_obj\n",
    "\n",
    "    gen = str(lm)\n",
    "    current_constraint = constraint\n",
    "\n",
    "    # TODO extend this to do a beam search\n",
    "    for i in range(10):\n",
    "        next_valid_tokens = recursive_getnext_valid_token(current_constraint)\n",
    "        if( len(next_valid_tokens) == 0):\n",
    "            break\n",
    "\n",
    "        next_token = gen_next_token(gen, next_valid_tokens) \n",
    "        next_chars = next_token # the token is returned as characters, not as a token id\n",
    "        gen += next_chars\n",
    "        current_constraint = current_constraint.set_next_chars(next_chars)\n",
    "\n",
    "    return gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emrek\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "lm = guidance.models.Transformers(\"gpt2\", temperature=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='margin: 0px; padding: 0px; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'>Hello<span style='background-color: rgba(0, 165, 0, 0.25)'>, I&#x27;m sorry, but I&#x27;m not sure</span></pre>"
      ],
      "text/plain": [
       "<guidance.models._transformers.Transformers at 0x1a4aafb9700>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm(\"Hello\").gen('next', max_tokens=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from guidance import constrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from guidance.library import DummyCharacterConstraint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='margin: 0px; padding: 0px; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'>Hello<span style='background-color: rgba(0, 165, 0, 0.25)'>Hello, Wow, Wow! Woooo, Wow!</span></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm = lm(\"Hello\").constrain(name = \"var1\", constraint=DummyCharacterConstraint(positiveConstraint=[' ', ',', 'W', 'w', 'o', 'r', 'l', 'd', '!'])) # 'I', '\\'', 'm', 'p', 'g', 'a', 'm', 'B', 'o', 'b', "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, Wow, Wow! Woooo, Wow!'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm['var1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='margin: 0px; padding: 0px; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'>Hello</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'OddEvenDigitConstraint' object has no attribute 'odds'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\emrek\\source\\guidance\\guidance-pythonic\\guidance\\emrek\\test-pythonic.ipynb Cell 17\u001b[0m line \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/emrek/source/guidance/guidance-pythonic/guidance/emrek/test-pythonic.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mguidance\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlibrary\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconstraints\u001b[39;00m \u001b[39mimport\u001b[39;00m OddEvenDigitConstraint\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/emrek/source/guidance/guidance-pythonic/guidance/emrek/test-pythonic.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m lm \u001b[39m=\u001b[39m guidance\u001b[39m.\u001b[39mmodels\u001b[39m.\u001b[39mTransformers(\u001b[39m\"\u001b[39m\u001b[39mgpt2\u001b[39m\u001b[39m\"\u001b[39m, temperature\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/emrek/source/guidance/guidance-pythonic/guidance/emrek/test-pythonic.ipynb#X22sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m lm \u001b[39m=\u001b[39m lm(\u001b[39m\"\u001b[39;49m\u001b[39mHello\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mconstrain(name \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mvar1\u001b[39;49m\u001b[39m\"\u001b[39;49m, constraint\u001b[39m=\u001b[39;49mOddEvenDigitConstraint(), max_tokens\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\users\\emrek\\source\\guidance\\guidance-pythonic\\guidance\\guidance\\__init__.py:114\u001b[0m, in \u001b[0;36m_decorator.<locals>._decorator_inner.<locals>.wrapper\u001b[1;34m(stream, async_mode, *args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[39mreturn\u001b[39;00m sync_iter_wrapper(lm, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    113\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m sync_wrapper(lm, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\users\\emrek\\source\\guidance\\guidance-pythonic\\guidance\\guidance\\__init__.py:66\u001b[0m, in \u001b[0;36m_decorator.<locals>._decorator_inner.<locals>.sync_wrapper\u001b[1;34m(lm, silent, hidden, *args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msync_wrapper\u001b[39m(lm, \u001b[39m*\u001b[39margs, silent\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, hidden\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     65\u001b[0m     \u001b[39mwith\u001b[39;00m Silent(lm, silent), optional_hidden(f, lm, hidden, kwargs):\n\u001b[1;32m---> 66\u001b[0m         \u001b[39mreturn\u001b[39;00m f(lm, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\users\\emrek\\source\\guidance\\guidance-pythonic\\guidance\\guidance\\library\\_constrain.py:136\u001b[0m, in \u001b[0;36mconstrain\u001b[1;34m(lm, name, constraint, suffix, logprobs, max_tokens, list_append)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[39m# TODO extend this to do a beam search, or if we have a small cheap model, we could use it for an A* search?\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_tokens):\n\u001b[1;32m--> 136\u001b[0m     next_valid_tokens \u001b[39m=\u001b[39m recursive_getnext_valid_token(current_constraint)\n\u001b[0;32m    137\u001b[0m     \u001b[39mif\u001b[39;00m( \u001b[39mlen\u001b[39m(next_valid_tokens) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[0;32m    138\u001b[0m         \u001b[39m# the only way to get here is if the constraint provider has reached the end of an option/program/etc\u001b[39;00m\n\u001b[0;32m    139\u001b[0m         \u001b[39m# TODO add ways for the LLM to end early, generate the suffix, etc.\u001b[39;00m\n\u001b[0;32m    140\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\users\\emrek\\source\\guidance\\guidance-pythonic\\guidance\\guidance\\library\\_constrain.py:97\u001b[0m, in \u001b[0;36mconstrain.<locals>.recursive_getnext_valid_token\u001b[1;34m(constraint, validstr)\u001b[0m\n\u001b[0;32m     94\u001b[0m     ret\u001b[39m.\u001b[39mappend(valid_tokens[\u001b[39m0\u001b[39m])\n\u001b[0;32m     95\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     96\u001b[0m     \u001b[39m# NOTE: adding the abbreviated token valid_tokens[0] when a longer one is available doesn't seem to work correctly ? TODO why\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m     next_constraint \u001b[39m=\u001b[39m constraint\u001b[39m.\u001b[39;49mset_next_chars(c)\n\u001b[0;32m     98\u001b[0m     recursive_valid_tokens \u001b[39m=\u001b[39m recursive_getnext_valid_token(next_constraint, validstr \u001b[39m+\u001b[39m c)\n\u001b[0;32m     99\u001b[0m     \u001b[39mif\u001b[39;00m( \u001b[39mlen\u001b[39m(recursive_valid_tokens) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m ):\n",
      "File \u001b[1;32mc:\\users\\emrek\\source\\guidance\\guidance-pythonic\\guidance\\guidance\\library\\constraints\\_testconstraints.py:35\u001b[0m, in \u001b[0;36mOddEvenDigitConstraint.set_next_chars\u001b[1;34m(self, c)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mset_next_chars\u001b[39m(\u001b[39mself\u001b[39m, c):\n\u001b[1;32m---> 35\u001b[0m     \u001b[39mif\u001b[39;00m( c[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49modds ):\n\u001b[0;32m     36\u001b[0m         \u001b[39mreturn\u001b[39;00m OddEvenDigitConstraint(prevDigitWasEven \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m     37\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'OddEvenDigitConstraint' object has no attribute 'odds'"
     ]
    }
   ],
   "source": [
    "from guidance.library.constraints import OddEvenDigitConstraint\n",
    "\n",
    "lm = guidance.models.Transformers(\"gpt2\", temperature=1.0)\n",
    "\n",
    "lm = lm(\"Hello\").constrain(name = \"var1\", constraint=OddEvenDigitConstraint(), max_tokens=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'c:\\\\users\\\\emrek\\\\source\\\\guidance\\\\guidance-pythonic\\\\guidance')\n",
    "import guidance\n",
    "from guidance.library.constraints import ContextFreeGrammarConstraint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='margin: 0px; padding: 0px; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'>Hello: <span style='background-color: rgba(0, 165, 0, 0.25)'>if (1) then X=1</span></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "lm = guidance.models.Transformers(\"gpt2\", temperature=1.0)\n",
    "\n",
    "testCFG = ContextFreeGrammarConstraint()\n",
    "testCFG.add_grammar_rule(testCFG.Rule(\"S\", [\"@i\", \"@f\", \"@ \", \"E\", \"@ \", \"@t\", \"@h\", \"@e\", \"@n\", \"@ \", \"C\"]))\n",
    "testCFG.add_grammar_rule(testCFG.Rule(\"E\", [\"@1\"]))\n",
    "testCFG.add_grammar_rule(testCFG.Rule(\"E\", [\"@2\"]))\n",
    "testCFG.add_grammar_rule(testCFG.Rule(\"E\", [\"@3\"]))\n",
    "testCFG.add_grammar_rule(testCFG.Rule(\"E\", [\"@4\"]))\n",
    "testCFG.add_grammar_rule(testCFG.Rule(\"E\", [\"@5\"]))\n",
    "testCFG.add_grammar_rule(testCFG.Rule(\"E\", [\"@6\"]))\n",
    "testCFG.add_grammar_rule(testCFG.Rule(\"E\", [\"@7\"]))\n",
    "testCFG.add_grammar_rule(testCFG.Rule(\"E\", [\"@8\"]))\n",
    "testCFG.add_grammar_rule(testCFG.Rule(\"E\", [\"@9\"]))\n",
    "testCFG.add_grammar_rule(testCFG.Rule(\"E\", [\"@(\", \"E\", \"@)\"]))\n",
    "testCFG.add_grammar_rule(testCFG.Rule(\"E\", [\"E\", \"@+\", \"E\"]))\n",
    "testCFG.add_grammar_rule(testCFG.Rule(\"E\", [\"E\", \"@-\", \"E\"]))\n",
    "testCFG.add_grammar_rule(testCFG.Rule(\"E\", [\"E\", \"@*\", \"E\"]))\n",
    "testCFG.add_grammar_rule(testCFG.Rule(\"E\", [\"E\", \"@/\", \"E\"]))\n",
    "testCFG.add_grammar_rule(testCFG.Rule(\"C\", [\"@X\", \"@=\", \"E\"]))\n",
    "testCFG.finalize_grammar()\n",
    "\n",
    "lm = lm(\"Hello: \").constrain(name = \"var1\", constraint=testCFG, max_tokens=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emrek\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "init_lm = guidance.models.Transformers(\"gpt2\", temperature=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='margin: 0px; padding: 0px; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'>If 2+4 then return 8: <span style='background-color: rgba(0, 165, 0, 0.25)'>The number of times the number of times the number of</span></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm = init_lm(\"If 2+4 then return 8: \").gen('next', max_tokens=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='margin: 0px; padding: 0px; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'>If 2+4 then return 8: <span style='background-color: rgba(0, 165, 0, 0.25)'>if 1 then X=6</span></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm = init_lm(\"If 2+4 then return 8: \").constrain(name = \"var1\", constraint=testCFG, max_tokens=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
