{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'c:\\\\users\\\\emrek\\\\source\\\\guidance\\\\guidance-pythonic\\\\guidance')\n",
    "\n",
    "import guidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environs import Env\n",
    "import os\n",
    "\n",
    "env = Env()\n",
    "env.read_env('./.env')\n",
    "\n",
    "# DONT FORGET TO CREATE THE .env file \n",
    "os.environ[\"OPENAI_API_TYPE\"] = env(\"OPENAI_API_TYPE\")\n",
    "# os.environ[\"OPENAI_API_VERSION\"] = env(\"OPENAI_API_VERSION\")\n",
    "# os.environ[\"OPENAI_API_BASE\"] = env(\"OPENAI_API_BASE\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='margin: 0px; padding: 0px; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'><div style='display: flex; border-bottom: 1px solid rgba(127, 127, 127, 0.2); align-items: center;'><div style='flex: 0 0 80px; opacity: 0.5;'>system</div><div style='flex-grow: 1; padding: 5px; padding-top: 10px; padding-bottom: 10px; margin-top: 0px; white-space: pre-wrap; margin-bottom: 0px;'>You are a helpful and terse assistant.</div></div><div style='display: flex; border-bottom: 1px solid rgba(127, 127, 127, 0.2); align-items: center;'><div style='flex: 0 0 80px; opacity: 0.5;'>user</div><div style='flex-grow: 1; padding: 5px; padding-top: 10px; padding-bottom: 10px; margin-top: 0px; white-space: pre-wrap; margin-bottom: 0px;'>Please tell me what the difference is between a frog and a toad.</div></div><div style='display: flex; border-bottom: 1px solid rgba(127, 127, 127, 0.2); align-items: center;'><div style='flex: 0 0 80px; opacity: 0.5;'>assistant</div><div style='flex-grow: 1; padding: 5px; padding-top: 10px; padding-bottom: 10px; margin-top: 0px; white-space: pre-wrap; margin-bottom: 0px;'><span style='background-color: rgba(0, 165, 0, 0.25)'>frogs are better</span></div></div></pre>"
      ],
      "text/plain": [
       "<guidance.models._openai.OpenAIChat at 0x2a8fddb77c0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = guidance.models.OpenAIChat(\"gpt-3.5-turbo\") #\"gpt-4\")\n",
    "\n",
    "lm = lm.system(\"You are a helpful and terse assistant.\")\n",
    "\n",
    "with lm.user() as lm:\n",
    "        lm += f\"\"\"Please tell me what the difference is between a frog and a toad.\"\"\"\n",
    "\n",
    "with lm.assistant() as lm:\n",
    "        #lm = lm.gen('answer', temperature=0, max_tokens=50)\n",
    "        lm = lm.select('answer', [\"toads are better\", \"frogs are better\"])\n",
    "        #lm = lm.select('answer', [\"toads are better\", \"frogs are better\", \"frogs are worse\"])\n",
    "        ## next line throws an error.\n",
    "        #lm = lm.select('answer', [\"frogs are better\", \"frogs are worse than toads\"])\n",
    "\n",
    "lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='margin: 0px; padding: 0px; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'><div style='display: flex; border-bottom: 1px solid rgba(127, 127, 127, 0.2); align-items: center;'><div style='flex: 0 0 80px; opacity: 0.5;'>system</div><div style='flex-grow: 1; padding: 5px; padding-top: 10px; padding-bottom: 10px; margin-top: 0px; white-space: pre-wrap; margin-bottom: 0px;'>You are a helpful and terse assistant.</div></div><div style='display: flex; border-bottom: 1px solid rgba(127, 127, 127, 0.2); align-items: center;'><div style='flex: 0 0 80px; opacity: 0.5;'>user</div><div style='flex-grow: 1; padding: 5px; padding-top: 10px; padding-bottom: 10px; margin-top: 0px; white-space: pre-wrap; margin-bottom: 0px;'>Please tell me what the difference is between a frog and a toad.</div></div><div style='display: flex; border-bottom: 1px solid rgba(127, 127, 127, 0.2); align-items: center;'><div style='flex: 0 0 80px; opacity: 0.5;'>assistant</div><div style='flex-grow: 1; padding: 5px; padding-top: 10px; padding-bottom: 10px; margin-top: 0px; white-space: pre-wrap; margin-bottom: 0px;'><span style='background-color: rgba(0, 165, 0, 0.25)'>Frogs and toads are both amphibians, but they have some differences. Frogs have smooth, moist skin and long legs for jumping, while toads have dry, bumpy skin and shorter legs for walking.</span></div></div></pre>"
      ],
      "text/plain": [
       "<guidance.models._openai.OpenAIChat at 0x1ab77736310>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = guidance.models.OpenAIChat(\"gpt-3.5-turbo\") #\"gpt-4\")\n",
    "\n",
    "lm = lm.system(\"You are a helpful and terse assistant.\")\n",
    "\n",
    "with lm.user() as lm:\n",
    "        lm += f\"\"\"Please tell me what the difference is between a frog and a toad.\"\"\"\n",
    "\n",
    "with lm.assistant() as lm:\n",
    "        lm = lm.gen('answer', temperature=0, max_tokens=50)\n",
    "        #lm = lm.select('answer', [\"toads are better\", \"frogs are better\"])\n",
    "        #lm = lm.select('answer', [\"toads are better\", \"frogs are better\", \"frogs are worse\"])\n",
    "        ## next line throws an error.\n",
    "        #lm = lm.select('answer', [\"frogs are better\", \"frogs are worse than toads\"])\n",
    "\n",
    "lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='margin: 0px; padding: 0px; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'> toad. <span style='background-color: rgba(0, 165, 0, 0.25)'>The</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "lm = guidance.models.Transformers(\"gpt2\", temperature=1.0)\n",
    "\n",
    "#lm += \"You are a helpful and terse assistant.\\n\"\n",
    "\n",
    "#lm += f\"\"\"Please tell me what the difference is between a frog and a toad. \"\"\"\n",
    "lm += \"\"\" toad. \"\"\"\n",
    "\n",
    "lm = lm.gen('answer', temperature=0, max_tokens=50)\n",
    "#lm = lm.select('answer', [\"Toads are better\", \"Frogs are better\", \"Frogs are worse\"])\n",
    "#lm = lm.select('answer', [\"toads are better\", \"frogs are better\", \"frogs are worse\"])\n",
    "## next line throws an error.\n",
    "#lm = lm.select('answer', [\"frogs are better\", \"frogs are worse than toads\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='margin: 0px; padding: 0px; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'>You are a helpful and terse assistant.\n",
       "Please tell me what the difference is between a frog and a toad. </pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "'generator' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\emrek\\source\\guidance\\guidance-pythonic\\guidance\\emrek\\test-pythonic.ipynb Cell 6\u001b[0m line \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/emrek/source/guidance/guidance-pythonic/guidance/emrek/test-pythonic.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m lm \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\"\"\u001b[39m\u001b[39mPlease tell me what the difference is between a frog and a toad. \u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/emrek/source/guidance/guidance-pythonic/guidance/emrek/test-pythonic.ipynb#W4sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m#lm = lm.gen('answer', temperature=0, max_tokens=50)\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/emrek/source/guidance/guidance-pythonic/guidance/emrek/test-pythonic.ipynb#W4sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m lm \u001b[39m=\u001b[39m lm\u001b[39m.\u001b[39;49mselect(\u001b[39m'\u001b[39;49m\u001b[39manswer\u001b[39;49m\u001b[39m'\u001b[39;49m, [\u001b[39m\"\u001b[39;49m\u001b[39mToads are better\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mFrogs are better\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mFrogs are worse\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n",
      "File \u001b[1;32mc:\\users\\emrek\\source\\guidance\\guidance-pythonic\\guidance\\guidance\\__init__.py:115\u001b[0m, in \u001b[0;36m_decorator.<locals>._decorator_inner.<locals>.wrapper\u001b[1;34m(stream, async_mode, *args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[39mreturn\u001b[39;00m sync_iter_wrapper(lm, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    114\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 115\u001b[0m     \u001b[39mreturn\u001b[39;00m sync_wrapper(lm, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\users\\emrek\\source\\guidance\\guidance-pythonic\\guidance\\guidance\\__init__.py:67\u001b[0m, in \u001b[0;36m_decorator.<locals>._decorator_inner.<locals>.sync_wrapper\u001b[1;34m(lm, silent, hidden, *args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msync_wrapper\u001b[39m(lm, \u001b[39m*\u001b[39margs, silent\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, hidden\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     66\u001b[0m     \u001b[39mwith\u001b[39;00m Silent(lm, silent), optional_hidden(f, lm, hidden, kwargs):\n\u001b[1;32m---> 67\u001b[0m         \u001b[39mreturn\u001b[39;00m f(lm, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\users\\emrek\\source\\guidance\\guidance-pythonic\\guidance\\guidance\\library\\_select.py:144\u001b[0m, in \u001b[0;36mselect\u001b[1;34m(lm, name, options, suffix, logprobs, list_append)\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[39mreturn\u001b[39;00m logprobs_out\n\u001b[0;32m    143\u001b[0m \u001b[39m# recursively compute the logprobs for each option\u001b[39;00m\n\u001b[1;32m--> 144\u001b[0m option_logprobs \u001b[39m=\u001b[39m recursive_select([])\n\u001b[0;32m    146\u001b[0m \u001b[39m# convert the key from a token list to a string\u001b[39;00m\n\u001b[0;32m    147\u001b[0m option_logprobs \u001b[39m=\u001b[39m {lm\u001b[39m.\u001b[39mget_decoded(k): v \u001b[39mfor\u001b[39;00m k,v \u001b[39min\u001b[39;00m option_logprobs\u001b[39m.\u001b[39mitems()}\n",
      "File \u001b[1;32mc:\\users\\emrek\\source\\guidance\\guidance-pythonic\\guidance\\guidance\\library\\_select.py:108\u001b[0m, in \u001b[0;36mselect.<locals>.recursive_select\u001b[1;34m(current_prefix, allow_token_extension)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[39m# generate the token logprobs\u001b[39;00m\n\u001b[0;32m    100\u001b[0m gen_obj \u001b[39m=\u001b[39m lm(pattern\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m# we don't need a pattern since we are just generating the next token\u001b[39;00m\n\u001b[0;32m    101\u001b[0m     max_tokens\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m    102\u001b[0m     logit_bias\u001b[39m=\u001b[39mlogit_bias,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    106\u001b[0m     token_healing\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m \u001b[39m# we manage token boundary healing ourselves for this function\u001b[39;00m\n\u001b[0;32m    107\u001b[0m )\n\u001b[1;32m--> 108\u001b[0m gen_obj \u001b[39m=\u001b[39m gen_obj[\u001b[39m\"\u001b[39;49m\u001b[39mchoices\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m0\u001b[39m] \u001b[39m# get the first choice (we only asked for one)\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mlogprobs\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m gen_obj:\n\u001b[0;32m    110\u001b[0m     logprobs_result \u001b[39m=\u001b[39m gen_obj[\u001b[39m\"\u001b[39m\u001b[39mlogprobs\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\u001b[1;31mTypeError\u001b[0m: 'generator' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "lm = guidance.models.Transformers(\"gpt2\", temperature=1.0)\n",
    "\n",
    "lm += \"You are a helpful and terse assistant.\\n\"\n",
    "\n",
    "lm += f\"\"\"Please tell me what the difference is between a frog and a toad. \"\"\"\n",
    "\n",
    "#lm = lm.gen('answer', temperature=0, max_tokens=50)\n",
    "lm = lm.select('answer', [\"Toads are better\", \"Frogs are better\", \"Frogs are worse\"])\n",
    "#lm = lm.select('answer', [\"toads are better\", \"frogs are better\", \"frogs are worse\"])\n",
    "## next line throws an error.\n",
    "#lm = lm.select('answer', [\"frogs are better\", \"frogs are worse than toads\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'Frogs are better'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm._variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='margin: 0px; padding: 0px; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'>Please tell me what the difference is between a frog and a toad. </pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "__call__() got an unexpected keyword argument 'logit_bias'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\emrek\\source\\guidance\\guidance-pythonic\\guidance\\emrek\\test-pythonic.ipynb Cell 7\u001b[0m line \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/emrek/source/guidance/guidance-pythonic/guidance/emrek/test-pythonic.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m lm \u001b[39m=\u001b[39m guidance\u001b[39m.\u001b[39mmodels\u001b[39m.\u001b[39mTransformers(\u001b[39m\"\u001b[39m\u001b[39mgpt2\u001b[39m\u001b[39m\"\u001b[39m, temperature\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/emrek/source/guidance/guidance-pythonic/guidance/emrek/test-pythonic.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m lm \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\"\"\u001b[39m\u001b[39mPlease tell me what the difference is between a frog and a toad. \u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/emrek/source/guidance/guidance-pythonic/guidance/emrek/test-pythonic.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m lm \u001b[39m=\u001b[39m lm\u001b[39m.\u001b[39;49mselect(\u001b[39m'\u001b[39;49m\u001b[39manswer\u001b[39;49m\u001b[39m'\u001b[39;49m, [\u001b[39m\"\u001b[39;49m\u001b[39mToads are better\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mFrogs are better\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mFrogs are worse\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n",
      "File \u001b[1;32mc:\\users\\emrek\\source\\guidance\\guidance-pythonic\\guidance\\guidance\\__init__.py:115\u001b[0m, in \u001b[0;36m_decorator.<locals>._decorator_inner.<locals>.wrapper\u001b[1;34m(stream, async_mode, *args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[39mreturn\u001b[39;00m sync_iter_wrapper(lm, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    114\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 115\u001b[0m     \u001b[39mreturn\u001b[39;00m sync_wrapper(lm, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\users\\emrek\\source\\guidance\\guidance-pythonic\\guidance\\guidance\\__init__.py:67\u001b[0m, in \u001b[0;36m_decorator.<locals>._decorator_inner.<locals>.sync_wrapper\u001b[1;34m(lm, silent, hidden, *args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msync_wrapper\u001b[39m(lm, \u001b[39m*\u001b[39margs, silent\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, hidden\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     66\u001b[0m     \u001b[39mwith\u001b[39;00m Silent(lm, silent), optional_hidden(f, lm, hidden, kwargs):\n\u001b[1;32m---> 67\u001b[0m         \u001b[39mreturn\u001b[39;00m f(lm, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\users\\emrek\\source\\guidance\\guidance-pythonic\\guidance\\guidance\\library\\_select.py:144\u001b[0m, in \u001b[0;36mselect\u001b[1;34m(lm, name, options, suffix, logprobs, list_append)\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[39mreturn\u001b[39;00m logprobs_out\n\u001b[0;32m    143\u001b[0m \u001b[39m# recursively compute the logprobs for each option\u001b[39;00m\n\u001b[1;32m--> 144\u001b[0m option_logprobs \u001b[39m=\u001b[39m recursive_select([])\n\u001b[0;32m    146\u001b[0m \u001b[39m# convert the key from a token list to a string\u001b[39;00m\n\u001b[0;32m    147\u001b[0m option_logprobs \u001b[39m=\u001b[39m {lm\u001b[39m.\u001b[39mget_decoded(k): v \u001b[39mfor\u001b[39;00m k,v \u001b[39min\u001b[39;00m option_logprobs\u001b[39m.\u001b[39mitems()}\n",
      "File \u001b[1;32mc:\\users\\emrek\\source\\guidance\\guidance-pythonic\\guidance\\guidance\\library\\_select.py:100\u001b[0m, in \u001b[0;36mselect.<locals>.recursive_select\u001b[1;34m(current_prefix, allow_token_extension)\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[39mreturn\u001b[39;00m logprobs_out\n\u001b[0;32m     99\u001b[0m \u001b[39m# generate the token logprobs\u001b[39;00m\n\u001b[1;32m--> 100\u001b[0m gen_obj \u001b[39m=\u001b[39m lm(pattern\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, \u001b[39m# we don't need a pattern since we are just generating the next token\u001b[39;49;00m\n\u001b[0;32m    101\u001b[0m     max_tokens\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m    102\u001b[0m     logit_bias\u001b[39m=\u001b[39;49mlogit_bias,\n\u001b[0;32m    103\u001b[0m     logprobs\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(logit_bias),\n\u001b[0;32m    104\u001b[0m     cache_seed\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[0;32m    105\u001b[0m     stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    106\u001b[0m     token_healing\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m \u001b[39m# we manage token boundary healing ourselves for this function\u001b[39;49;00m\n\u001b[0;32m    107\u001b[0m )\n\u001b[0;32m    108\u001b[0m gen_obj \u001b[39m=\u001b[39m gen_obj[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m] \u001b[39m# get the first choice (we only asked for one)\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mlogprobs\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m gen_obj:\n",
      "\u001b[1;31mTypeError\u001b[0m: __call__() got an unexpected keyword argument 'logit_bias'"
     ]
    }
   ],
   "source": [
    "lm = guidance.models.Transformers(\"gpt2\", temperature=1.0)\n",
    "\n",
    "lm += f\"\"\"Please tell me what the difference is between a frog and a toad. \"\"\"\n",
    "lm = lm.select('answer', [\"Toads are better\", \"Frogs are better\", \"Frogs are worse\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
